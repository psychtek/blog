install_github("willemsleegers/tidystats")
library(tidystats)
knitr::opts_chunk$set(echo = TRUE)
# A script to scrape the top comments from the top posts of Reddit or a specified
# subreddit and create a word frequency table from them. It also can plot a
# word cloud from the most common words on the page.
# Notes:
#
# This won't work on NSFW subreddits that require you to click that you're 18
# or older. I haven't looked at how to code that yet.
# Overview of the function:
# 1. Get the selected page (front or subreddit)
# 2. Build the links to all of the comments
# 3. Scrape each comments page (this step can take a while, 10 to 40 seconds)
# 4. Clean it up
# 5. Get the word frequency
# 6. Plot a word cloud
# 7. Return a word frequency table
redditScrape <- function(subred = c('nameOfSubred', 'allTop'), time = c('day', 'week', 'month', 'year'), plotCloud = TRUE, saveText = FALSE, myDirectory = "/choose/a/directory") {
#######################################################
# 0. Load the required packages.  And check a few items
require(XML)
require(RCurl)
require(RColorBrewer)
require(wordcloud)
# if more than one time, apply function to each time frame separately
if (length(time) > 1) {
return(lapply(time, function(i)
redditScrape(subred=subred, time=i, saveText=saveText, myDirectory=myDirectory)))
}
#######################################################
# 1. Make the url, get the page.
if (subred == 'allTop') {
url <- paste('http://www.reddit.com/top/?sort=top&t=', time, sep = "")
} else {
url <- paste("http://www.reddit.com/r/", subred, "/top/?sort=top&t=", time, sep = "")
}
doc <- htmlParse(url)
#######################################################
# 2. Get the links that go to comment sections of the posts
links <- xpathSApply(doc, "//a/@href")
comments <- grep("comments", links)
comLinks <- links[comments]
comments <- grep('reddit.com', comLinks, fixed=TRUE)
comLinks <- comLinks[comments]
#######################################################
#  3. Scrape the pages
#  This will scrape a page and put it in to
#  an R list object
textList <- as.list(rep(as.character(""), length(comLinks)))
docs <- getURL(comLinks)
for (i in 1:length(docs)) {
textList[[i]] <- htmlParse(docs[i], asText = TRUE)
textList[[i]] <- xpathSApply(textList[[i]], "//p", xmlValue)
}
#######################################################
#  4. Clean up the text.
# Remove the submitted lines and lines at the end of each page
for (i in 1:length(textList)) {
submitLine <- grep("submitted [0-9]", textList[[i]])
textList[[i]] <- textList[[i]][{(submitLine[1] + 1):(length(textList[[i]])-10)}]
}
# Removing lines capturing user and points, etc.
# Yes, there could be fewer grep calls, but this made it
# easier to keep track of what was going on.
textList <- lapply(textList, function(i){
grep('points 1 minute ago', i) -> nameLines1
grep('points [0-9] minutes ago', i) -> nameLines2
grep('points [0-9][0-9] minutes ago', i) -> nameLines3
grep("points 1 hour ago", i) -> nameLines4
grep("points [0-9] hours ago", i) -> nameLines5
grep("points [0-9][0-9] hours ago", i) -> nameLines6
grep('points 1 day ago', i) -> nameLines7
grep('points [0-9] days ago', i) -> nameLines8
grep('points [0-9][0-9] days ago', i) -> nameLines9
grep('points 1 month ago', i) -> nameLines10
grep('points [0-9] months ago', i) -> nameLines11
grep('points [0-9][0-9] months ago', i) -> nameLines12
allLines <- c(nameLines1, nameLines2, nameLines3, nameLines4,
nameLines5, nameLines6, nameLines7, nameLines8, nameLines9,
nameLines10, nameLines11, nameLines12)
temp <- i[-allLines]
temp <- temp[temp!=""]
tolower(temp)
})
# Let's simplify our list. Could have been done earlier, but so it goes.
allText <- unlist(textList)
# Remove the punctuation, links, etc.
allText <- gsub("https?://[[:alnum:][:punct:]]+", "", allText)
allText <- gsub("[,.!?\"]", "", allText)
allText <- strsplit(allText, "\\W+", perl=TRUE)
allText <- unlist(allText)
# Remove frequent words and orphans of contractions (that sounds
# sadder than it is).
frequentWords <- c("the", "be", "been", "to", "of", "and", "a", "in",
"that", "have", "i", "it", "for", "not", "on", "with", "he", "as", "you",
"do", "at", "this", "but", "his", "by", "from", "they", "we", "say", "her",
"she", "or", "an", "will", "my", "one", "all", "would", "there", "their",
"what", "so", "up", "out", "if", "about", "who", "get", "which", "go",
"me", "when", "make", "can", "like", "time", "no", "just", "him", "know",
"take", "people", "into", "year", "your", "good", "some", "could", "them",
"see", "other", "than", "then", "now", "look", "only", "come", "its",
"over", "think", "also", "back", "after", "use", "two", "how", "our",
"work", "first", "well", "way", "even", "new", "want", "because", "any",
"these", "give", "day", "most", "us", 'is', 'are', 'was', 'were', 'i', 's',
'was', 'don', 'aren', 'points1', 'point', 't', 'm', 'points0', '10', '1',
're', 'll', 'd', '2', '3', '4', '5', '6', '7', '8', '9', 'doesn','d', 've',
'r', 'has', 'had', 'been', 'being', '0', 'more', 'really', 'isn', 'very',
'am', 'didn', 'wouldn', '', 'points', 'point', 'months', 'ago', 'deleted',
'much')
for (i in 1:length(frequentWords)) {
allText <- allText[allText!=frequentWords[i]]
}
# Save the file to your drive. This way you can drop it into
# Wordle.net or use it other places.
if (saveText == TRUE) {
curWD <- getwd()
setwd(myDirectory)
filename <- paste("Reddit Comments Postscrub ", subred, " ", time, " ",
Sys.time(),".txt", sep = "")
write.table(allText, file = filename, row.names=F, col.names=F, append=T)
# save(allText, file = filename)
textListBackup <- textList
setwd(curWD)
}
#######################################################
#  5. Word frequency table
textTable <- table(allText)
textTable <- sort(textTable, decreasing = TRUE)
#######################################################
#  6. Plot word cloud
if (plotCloud == TRUE) {
# This is a nice option.  Just use a portion of the 0-1 for color
rainbow(30,s=.8,v=.6,start=.5,end=1,alpha=1) -> pal
wordcloud(names(textTable[1:200]), textTable[1:200], scale = c(4,.5), max.words = 200, colors = pal)
}
#######################################################
#  7. Return the text table
textTable
}
test <- redditScrape(subred = c("ADHD", "allTop", time = c("year", plotCloud = TRUE, saveText = TRUE, myDirectory = "dcontent/post/data/")))
test <- redditScrape(subred = c("ADHD", "allTop", time = c("day", plotCloud = TRUE, saveText = TRUE, myDirectory = "dcontent/post/data/")))
install.packages("RedditExtractoR")
library(RedditExtractoR)
reddit_content(URL = "https://www.reddit.com/r/ADHD/top/", wait_time = 5)
reddit_content(URL = "https://www.reddit.com/r/ADHD/top/")
reddit_content(URL = "https://www.reddit.com/r/ADHD/")
reddit_links <- reddit_urls(
search_terms   = "ADHD",
page_threshold = 1
)
reddit_link
reddit_links
install.packages(c("curl", "DescTools", "OpenMx", "raster", "TTR"))
library(tidyverse)
global_mort <- read_csv(file = "data/global_mortality.csv", col_names = TRUE) # import dataset into object
global_mort$country <- as.factor(global_mort$country) # Change character to factors
global_mort$country_code <- as.factor(global_mort$country_code)
global_mort$year <- as.factor(global_mort$year)
library(hrbrthemes)
Australia <- global_mort %>%
group_by(country, year) %>%
arrange(year) %>%
filter(country == "Australia")
ggplot(Australia) +
aes(x = year, weight = `Cardiovascular diseases (%)`) +
geom_bar(fill = ft_cols$blue) +
labs(x = "", y = "Amount %", title = "Cardiovascular Disease in Australia", subtitle = "1990 to 2016") +
theme_ft_rc()
# Drug Related Deaths in Australia
ggplot(Australia) +
aes(x = year, weight = `Drug disorders (%)`) +
geom_bar(fill = ft_cols$blue) +
labs(x = "", y = "Amount %", title = "Drug Related Deaths in Australia", subtitle = "1990 to 2016") +
theme_ft_rc()
ggplot(Australia) +
aes(x = year, weight = `Suicide (%)`) +
geom_bar(fill = ft_cols$blue) +
labs(x = "", y = "Amount %", title = "Suicides in Australia", subtitle = "1990 to 2016") +
theme_ft_rc()
Australia
Australia %>% view()
ggplot(Australia) +
aes(x = year, weight = 'Alcohol disorders (%)' ) +
geom_bar(fill = ft_cols$blue) +
labs(x = "", y = "Amount %", title = "Suicides in Australia", subtitle = "1990 to 2016") +
theme_ft_rc()
library(hrbrthemes)
ggplot(Australia) +
aes(x = year, weight = 'Alcohol disorders (%)' ) +
geom_bar(fill = ft_cols$blue) +
labs(x = "", y = "Amount %", title = "Suicides in Australia", subtitle = "1990 to 2016") +
theme_ft_rc()
ggplot(Australia) +
aes(x = year, weight = `Alcohol disorders (%)` ) +
geom_bar(fill = ft_cols$blue) +
labs(x = "", y = "Amount %", title = "Alochol Use in Australia", subtitle = "1990 to 2016") +
theme_ft_rc()
library(plotly)
a%%b
a <- c(1.2,2,3.5,4)
b <-  c(1.2,2.2,3.5,4)
csum <- sum(a == b)
csum
ls()
list.objects()
?mtrx
knitr::opts_chunk$set(echo = TRUE)
library(RedditExtractoR)
reddit_links <- reddit_urls(
search_terms   = "diagnosed"
, page_threshold = 1
, subreddit = "ADHD"
)
reddit_links %>%
view()
library(tidyverse)
reddit_links %>%
view()
reddit_links <- reddit_urls(
search_terms   = c("diagnosed")
, page_threshold = 1
, subreddit = "ADHD"
)
reddit_links <- reddit_urls(
search_terms   = c("diagnosed", "Australia", "Australian")
, page_threshold = 1
, subreddit = "ADHD"
)
install.packages("jsonlite")
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
page <- fromJSON(url("https://www.reddit.com/r/ADHD/top/"),flatten = T)
reddit_links <- reddit_urls(
search_terms   = c("diagnosed")
, page_threshold = 3
, subreddit = "ADHD"
)
library(RedditExtractoR)
reddit_links <- reddit_urls(
search_terms   = c("diagnosed")
, page_threshold = 3
, subreddit = "ADHD"
)
reddit_links %>%
view()
library(tidyverse)
reddit_links %>%
view()
str(reddit_links)
page <- fromJSON(url("https://www.reddit.com/r/ADHD/top/.json"),flatten = T)
df <- page$data$children
df
page %>%
view()
knitr::opts_chunk$set(echo = TRUE)
runif(n = 5, min = 1, max = 25)
runif(n = 50, min = 1, max = 25)
hist(runif(n = 50, min = 1, max = 25))
hist(runif(n = 5, min = 1, max = 25))
hist(runif(n = 500, min = 1, max = 25))
hist(runif(n = 5000, min = 1, max = 25))
hist(rnorm(n = 50, mean = 5, sd = 1))
hist(rnorm(n = 5000, mean = 5, sd = 1))
hist(rnorm(n = 43, mean = 4.5, sd = 1.5))
install.packages(c("backports", "blavaan", "blogdown", "bookdown", "curl", "DescTools", "glasso", "inspectdf", "OpenMx", "pagedown", "raster", "RcppParallel", "rmarkdown", "rticles", "TTR", "xfun"))
knitr::opts_chunk$set(echo = TRUE)
remotes::install_github("mkearney/rreddit")
library(rreddit)
d <- get_r_reddit("ADHD", n = 100)
d %>% View()
library(tidyverse)
d %>% View()
comments <- get_comment_reddit(subreddit = "ADHD", author = NULL, n = 100)
remotes::install_github("mkearney/tbltools")
comments <- get_comment_reddit(subreddit = "ADHD", author = NULL, n = 100)
trace("get_r_reddit", edit = TRUE)
comments <- get_comment_reddit(subreddit = "ADHD", author = NULL, n = 100)
trace("get_r_reddit", edit = TRUE)
library(rreddit)
comments <- get_comment_reddit(subreddit = "ADHD", author = NULL, n = 100)
trace("get_r_reddit", edit = TRUE)
d$selftext[,1]
d$selftext[1,]
d$selftext[1]
d$selftext[3]
View(d)
d %>%  select(author, created_utc, num_comments, selftext)
d %>% filter(is_video == "TRUE")
d %>%
select(author,num_comments, selftext)
adhd <- d %>%
select(author,num_comments, selftext)
adhd
adhd %>%
count(num_comments)
adhd
adhd %>%
group_by(num_comments) %>%
count(num_comments)
adhd$num_comments
View(adhd)
adhd %>% select(selftext)
library("tm")
library("SnowballC")
library("tm")
library("wordcloud")
library("RColorBrewer")
install.packages("tm")
library("tm")
detach("tm")
detach("package:tm", unload = TRUE)
install.packages("tm")
library("tm")
install.packages("tm")
install.packages(c("backports", "inspectdf", "RJSONIO", "rstanarm"))
library(tidyverse)
global_mort <- read_csv(file = "data/global_mortality.csv", col_names = TRUE) # import dataset into object
global_mort$country <- as.factor(global_mort$country) # Change character to factors
global_mort$country_code <- as.factor(global_mort$country_code)
global_mort$year <- as.factor(global_mort$year)
library(hrbrthemes) # custom dark theme from hrbrpackage
Australia <- global_mort %>%
group_by(country, year) %>%
arrange(year) %>%
filter(country == "Australia")
ggplot(Australia) +
aes(x = year, weight = `Cardiovascular diseases (%)`) +
geom_bar(fill = ft_cols$blue) +
labs(x = "", y = "Amount %", title = "Cardiovascular Disease in Australia", subtitle = "1990 to 2016") +
theme_ft_rc()
# Drug Related Deaths in Australia
ggplot(Australia) +
aes(x = year, weight = `Drug disorders (%)`) +
geom_bar(fill = ft_cols$blue) +
labs(x = "", y = "Amount %", title = "Drug Related Deaths in Australia", subtitle = "1990 to 2016") +
theme_ft_rc()
ggplot(Australia) +
aes(x = year, weight = `Suicide (%)`) +
geom_bar(fill = ft_cols$blue) +
labs(x = "", y = "Amount %", title = "Suicides in Australia", subtitle = "1990 to 2016") +
theme_ft_rc()
ggplot(Australia) +
aes(x = year, weight = `Alcohol disorders (%)` ) +
geom_bar(fill = ft_cols$blue) +
labs(x = "", y = "Amount %", title = "Alochol Use in Australia", subtitle = "1990 to 2016") +
theme_ft_rc()
names(Australia)
library(plotly)
global_long <- global_mort %>%
group_by(country, year) %>%
janitor::clean_names() %>%  # Janitor package will clean the names to snake format and the percent values
gather(death, value, contains("percent"))
global_long$death <- as.factor(global_long$death)
global_long %>% group_by(country, death) %>% summarise(X = mean(value/(27)))
df <- global_mort %>%
group_by(country, year) %>%
pivot_longer(cols = c(4:35))
df
df %>%
filter("Australia" %in% country) %>%
filter("Alcohol disorders (%)" == name) %>%
#top_n(-15) %>%
ggplot(aes(x = year, y = value, group = name)) +
geom_line(aes(color = name), size = 1) +
labs(x = "Year", y = "Percentage (%)", title = "Australian Mortality Rates", subtitle = "1990 to 2016") +
theme_ft_rc() +
theme(axis.text.x = element_text(size=10, angle=45)) +
scale_y_percent()
df %>%
filter("Australia" %in% country) %>%
filter("Alcohol disorders (%)" == name) %>%
#top_n(-15) %>%
ggplot(aes(x = year, y = value, group = name)) +
geom_line(aes(color = name), size = 1) +
labs(x = "Year", y = "Percentage (%)", title = "Australian Mortality Rates", subtitle = "1990 to 2016") +
theme_ft_rc() +
theme(axis.text.x = element_text(size=10, angle=45)) +
scale_y_percent()
df
df %>%
filter("Australia" %in% country) %>%
#filter("Alcohol disorders (%)" == name) %>%
#top_n(-15) %>%
ggplot(aes(x = year, y = value, group = name)) +
geom_line(aes(color = name), size = 1) +
labs(x = "Year", y = "Percentage (%)", title = "Australian Mortality Rates", subtitle = "1990 to 2016") +
theme_ft_rc() +
theme(axis.text.x = element_text(size=10, angle=45)) +
scale_y_percent()
df %>%
filter("Australia" %in% country) %>%
group_by(year, name) %>%
filter(name == "Alcohol disorders (%)")
df %>%
filter("Australia" %in% country) %>%
#filter("Alcohol disorders (%)" == name) %>%
#top_n(-15) %>%
ggplot(aes(x = year, y = value, group = name)) +
geom_line(aes(color = name), size = 1) +
labs(x = "Year", y = "Percentage (%)", title = "Australian Mortality Rates", subtitle = "1990 to 2016") +
theme_ft_rc() +
theme(axis.text.x = element_text(size=10, angle=45)) +
scale_y_percent()
# data_df <- read.csv(file = "https://query.data.world/s/iwzl7xgiagpimb2ixfj4fkyztngxs7", stringsAsFactors = FALSE, header = TRUE)
#write.csv(data_df, file = "data/nutrian_df.csv") # write the dataframe to csv
data_df <- read_csv(file = "data/nutrian_df.csv")
data_df <- data_df %>% select(-contains("_USRDA"))
data_df$FoodGroup <- factor(data_df$FoodGroup, ordered = is.ordered(data_df$FoodGroup)) # coerce to factor
Foodgroup_cat <- data_df %>% # Group by the food group catagory and count the amont each one occurs
group_by(FoodGroup) %>%
summarise(N = n()) %>%
arrange(N)
Foodgroup_cat$FoodGroup <- str_remove(Foodgroup_cat$FoodGroup, "Products") # Remove the word "products" from the list
ggplot(Foodgroup_cat) +
aes(x = reorder(FoodGroup, -N), y = N) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Toal Amount of Food Products",
subtitle = "USDRA",
x = "Products",
y = "Frequency")
ggplot(Foodgroup_cat) +
aes(x = reorder(FoodGroup, -N), y = N) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Toal Amount of Food Products",
subtitle = "USDRA",
x = "Products",
y = "Frequency") +
theme_ft_rc()
ggplot(data_df) +
aes(x = Sugar_g, y = Energy_kcal, fill = FoodGroup, colour = FoodGroup) +
geom_point(size = 1L) +
scale_fill_hue() +
scale_color_hue() +
labs(x = "Sugar in gram", y = "Energy in k/cals", title = "Amount of Energy from Sugar for each Food Group", subtitle = "Food Groups") +
theme_ft_rc() +
theme(legend.position = "none") +
facet_wrap(vars(FoodGroup), scales = "free_x")
data_df[,9:31] <- sqrt(data_df[,9:31]) # sqrt transform
data_df_rescale <- data_df %>%
select(9:31) %>%
scale(center = TRUE)
data_df_corr <- data_df %>% select(9:31)
data_corrr <- cor(data_df_corr)
corrplot(as.matrix(data_corrr), is.corr = FALSE, method = "square", type = "full")
data_corrr <- cor(data_df_corr)
data_corrr
library(corrplot)
corrplot(as.matrix(data_corrr), is.corr = FALSE, method = "square", type = "full")
data_PCA <- prcomp(data_df_rescale)
data_PCA
summary(data_PCA)
tidy(summary(data_PCA))
plot(data_PCA, type = "l")
library(FactoMineR)
data_PCA_all <- PCA(data_df_rescale, ncp  = 5, graph = TRUE)
data_PCA_all <- PCA(data_df_rescale, ncp  = 2, graph = TRUE)
library(factoextra)
# a more colourful visual representation of the variables and where they converge on a component.
fviz_pca_var(data_PCA_all
, col.var = "cos2"
, gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
, repel = TRUE) # Avoid text overlapping)
corrplot(data_PCA_all$var$cor, is.corr=FALSE)
fviz_contrib(data_PCA_all, choice = "var", axes = 1:2, top = 10)
get_eig(data_PCA_all)
fviz_eig(data_PCA_all)
library(factoextra)
# a more colourful visual representation of the variables and where they converge on a component.
fviz_pca_var(data_PCA_all
, col.var = "cos2"
, gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
, repel = TRUE) # Avoid text overlapping)
corrplot(data_PCA_all$var$cor, is.corr=FALSE)
fviz_contrib(data_PCA_all, choice = "var", axes = 1:2, top = 10)
get_eig(data_PCA_all)
fviz_eig(data_PCA_all)
library(factoextra)
# a more colourful visual representation of the variables and where they converge on a component.
fviz_pca_var(data_PCA_all
, col.var = "cos2"
, gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
, repel = TRUE) # Avoid text overlapping)
get_eig(data_PCA_all)
fviz_eig(data_PCA_all)
corrplot(data_PCA_all$var$cor, is.corr=FALSE)
fviz_contrib(data_PCA_all, choice = "var", axes = 1:2, top = 10)
fviz_nbclust(data_PCA_all)
df <- global_mort %>%
group_by(country, year) %>%
pivot_longer(cols = c(4:35))
df$name <- factor(df$name)
df %>%
filter("Australia" %in% country) %>%
#filter("Alcohol disorders (%)" == name) %>%
#top_n(-15) %>%
ggplot(aes(x = year, y = value, group = name)) +
geom_line(aes(color = name), size = 1) +
labs(x = "Year", y = "Percentage (%)", title = "Australian Mortality Rates", subtitle = "1990 to 2016") +
theme_ft_rc() +
theme(axis.text.x = element_text(size=10, angle=45)) +
scale_y_percent()
# ggplotly(p)
df %>%
filter("Australia" %in% country) %>%
group_by(year, name) %>%
filter(name == "Alcohol disorders (%)")
View(df)
blogdown:::serve_site()
blogdown::stop_server()
